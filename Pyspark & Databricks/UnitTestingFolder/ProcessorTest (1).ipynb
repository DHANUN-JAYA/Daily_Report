{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b6291d-6007-4773-ba54-b6af8106a139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 'path=\\'dbfs:/FileStore/annual_enterprise_survey_2021_financial_year_provisional_csv.csv\\'\\nformat_name=\\'csv\\'\\nschema={\"Header\":True,\"InferSchema\":True}'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9519e8b6-215a-4422-a38a-a58bb53f762b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/sadhu.dhanunjay@diggibyte.com/unitTestingFolder/mainCommonFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba6ad83-eab9-4559-bb1f-8e928802ebb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime \n",
    "import logging\n",
    "class test_duplicate_primary_key:\n",
    "    def __init__(self):\n",
    "        self.d = {}\n",
    "    def test_duplicate_primary_key_(self,df,primary_key_columns):\n",
    "        duplicate_count = df.groupBy(primary_key_columns).count().filter(\"count > 1\").count()\n",
    "        if duplicate_count==0:\n",
    "            self.d[f\"Test Case  for test_duplicate_primary_key, Params:{primary_key_columns}\"]=\"passed\"\n",
    "            \n",
    "        else:\n",
    "            self.d[f\"Test Case  for test_duplicate_primary_key, Params:{primary_key_columns}\"]=\"failed\"\n",
    "    def test_non_null_primary_key(self,df,primary_key_columns):\n",
    "        \n",
    "        null_count = df.where(df[primary_key_columns].isNull()).count()\n",
    "        if null_count==0:\n",
    "            self.d[f\"Test Case  for test_non_null_primary_key, Params:{primary_key_columns}\"]=\"passed\"\n",
    "            \n",
    "        else:\n",
    "            self.d[f\"Test Case for test_non_null_primary_key, Params:{primary_key_columns}\"]=\"failed\"\n",
    "        \n",
    "    def test_records_going_to_quarantine_path(self,df):\n",
    "        quarantined_count = df.where(df[\"quarantine_flag\"] == True).count()\n",
    "        if quarantined_count==0:\n",
    "            self.d[f\"Test Case for test_records_going_to_quarantine_path \"]=\"passed\"\n",
    "            \n",
    "        else:\n",
    "            self.d[f\"Test Case for test_records_going_to_quarantine_path \"]=\"failed\"\n",
    "\n",
    "    def test_audit_column(self,silver_df,gold_df):\n",
    "        if gold_df.schema['load_date'].dataType == TimestampType() and silver_df.schema['load_date'].dataType == TimestampType():\n",
    "            self.d[f\"Test Case for test_audit_column \"]=\"passed\"\n",
    "        else:\n",
    "            self.d[f\"Test Case for test_audit_column \"]=\"failed\"\n",
    "        \n",
    "    def test_file_format(self,raw_location,bronze_location):\n",
    "        raw_location=\"dbfs:/FileStore/Nested_json_file__1_-1.json\"\n",
    "        bronze_location=\"dbfs:/FileStore/Nested_json_file__1_-1.json\"\n",
    "        raw_format=raw_location.split('.')[-1]\n",
    "        bronze_format=bronze_location.split('.')[-1]\n",
    "        if raw_format==bronze_format:\n",
    "            self.d[f\"Test Case for test_audit_column, params : path raw_location={raw_location},bronze_location={bronze_location}\"]=\"passed\"\n",
    "            \n",
    "        else:\n",
    "            self.d[f\"Test Case for test_audit_column, params : path raw_location={raw_location},bronze_location={bronze_location}\"]=\"failed\"\n",
    "        \n",
    "    def test_data_ingestion(self,df):\n",
    "        # Verify data ingestion\n",
    "        if len(df.columns)==len(custom_schema):\n",
    "            self.d[f\"Test Case for test_data_ingestion \"]=\"passed\"\n",
    "            \n",
    "        else:\n",
    "            self.d[f\"Test Case for test_data_ingestion \"]=\"failed\"\n",
    "\n",
    "    def print_results(self):\n",
    "        li=[]\n",
    "        for i in self.d:\n",
    "            li.append((i,self.d[i]))\n",
    "        df=spark.createDataFrame(li,[\"test_name\",\"status\"])\n",
    "        df.display()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# s = unittest.TestLoader().loadTestsFromTestCase(test_)\n",
    "# unittest.TextTestRunner(verbosity=2).run(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53bc138e-781b-40dd-84a6-6305d8ae51b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case Passed for test_duplicate_primary_key Passed\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-944103726745948>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m primary_key_columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m      6\u001B[0m c\u001B[38;5;241m=\u001B[39mtest_duplicate_primary_key()\n",
       "\u001B[0;32m----> 7\u001B[0m b\u001B[38;5;241m=\u001B[39mc\u001B[38;5;241m.\u001B[39mtest_duplicate_primary_key_(df,primary_key_columns)\n",
       "\n",
       "File \u001B[0;32m<command-3249066694297884>:16\u001B[0m, in \u001B[0;36mtest_duplicate_primary_key.test_duplicate_primary_key_\u001B[0;34m(self, df, primary_key_columns)\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m duplicate_count\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Case Passed for test_duplicate_primary_key Passed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 16\u001B[0m     \u001B[43md\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Case Failed for test_duplicate_primary_key Passed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'd' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-944103726745948>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m primary_key_columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m c\u001B[38;5;241m=\u001B[39mtest_duplicate_primary_key()\n\u001B[0;32m----> 7\u001B[0m b\u001B[38;5;241m=\u001B[39mc\u001B[38;5;241m.\u001B[39mtest_duplicate_primary_key_(df,primary_key_columns)\n\nFile \u001B[0;32m<command-3249066694297884>:16\u001B[0m, in \u001B[0;36mtest_duplicate_primary_key.test_duplicate_primary_key_\u001B[0;34m(self, df, primary_key_columns)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m duplicate_count\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Case Passed for test_duplicate_primary_key Passed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m     \u001B[43md\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Case Failed for test_duplicate_primary_key Passed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'd' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'd' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# path=\"dbfs:/FileStore/tbro\"\n",
    "# format_name=\"csv\"\n",
    "# options={\"Header\":True,\"InferSchema\":True}\n",
    "# df=create_df(path,format_name,**options)\n",
    "# primary_key_columns = ['name']\n",
    "# c=test_duplicate_primary_key()\n",
    "# b=c.test_duplicate_primary_key_(df,primary_key_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f9c9622-5589-4d56-a908-535979bf4366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a=test_duplicate_primary_key()\n",
    "# b=a.print_d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8955d59e-7c1e-4d35-9dd4-336956515a1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import unittest\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *\n",
    "# from datetime import datetime \n",
    "# class test_duplicate_primary_key(unittest.TestCase):\n",
    "#     def __init__(self, methodName='runTest', df=None, primary_key_columns=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.df = df\n",
    "#         self.primary_key_columns = primary_key_columns\n",
    "#     def test_duplicate_primary_key_(self):\n",
    "#         duplicate_count = duplicate_primary_key(df,primary_key_columns)\n",
    "#         self.assertEqual(duplicate_count, 0)\n",
    "#     def __init__(self, methodName='runTest', df=None, primary_key_columns=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.df = df\n",
    "#         self.primary_key_columns = primary_key_columns\n",
    "    \n",
    "#     def test_non_null_primary_key(self):\n",
    "#         path=\"dbfs:/FileStore/tbro\"\n",
    "#         format_name=\"csv\"\n",
    "#         options={\"Header\":True,\"InferSchema\":True}\n",
    "#         df=create_df(path,format_name,**options)\n",
    "#         primary_key_columns = 'name'\n",
    "#         null_count = non_null_primary_key(df,primary_key_columns)\n",
    "#         self.assertEqual(null_count, 0)\n",
    "#     def __init__(self, methodName='runTest', df=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.df = df\n",
    "#     def test_records_going_to_quarantine_path(self):\n",
    "#         quarantined_count = records_going_to_quarantine_path(df)\n",
    "#         self.assertGreater(quarantined_count, 0)\n",
    "#     def __init__(self, methodName='runTest', silver_df=None,gold_df=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.silver_df = silver_df\n",
    "#         self.gold_df=gold_df\n",
    "#     def test_audit_column(self):\n",
    "#         bool_value=audit_column(silver_df,gold_df)\n",
    "#         self.assertEqual(bool_value, True)\n",
    "#     def __init__(self, methodName='runTest', raw_location=None,bronze_format=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.raw_location = raw_location\n",
    "#         self.bronze_format=bronze_format\n",
    "#     def test_file_format(self):\n",
    "#         raw_location=\"dbfs:/FileStore/Nested_json_file__1_-1.json\"\n",
    "#         bronze_location=\"dbfs:/FileStore/Nested_json_file__1_-1.json\"\n",
    "#         raw_format=get_file_format(raw_location)\n",
    "#         bronze_format=get_file_format(bronze_location)\n",
    "#         self.assertEqual(raw_format, bronze_format)\n",
    "#     def __init__(self, methodName='runTest', df=None,custom_schema=None):\n",
    "#         super(test_duplicate_primary_key, self).__init__(methodName)\n",
    "#         self.df = df\n",
    "#         self.custom_schema=custom_schema\n",
    "#     def test_data_ingestion(self):\n",
    "#         # Verify data ingestion\n",
    "#         self.assertEqual(len(df.columns), len(custom_schema))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# # s = unittest.TestLoader().loadTestsFromTestCase(test_)\n",
    "# # unittest.TextTestRunner(verbosity=2).run(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b081b4-67d6-436f-8cf1-28153e5ed286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597a6137-acc0-4b89-87f0-dcc0873254a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>30</td></tr><tr><td>John</td><td>25</td></tr><tr><td>Bob</td><td>35</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         30
        ],
        [
         "John",
         25
        ],
        [
         "Bob",
         35
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# path=\"dbfs:/FileStore/tbro\"\n",
    "# format_name=\"csv\"\n",
    "# options={\"Header\":True,\"InferSchema\":True}\n",
    "# df=create_df(path,format_name,**options)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9a87c0-0e81-41e6-a488-a71b572b9f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# df.write.format(\"delta\").save('/FileStore/sampledat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68aa3012-f839-49e6-bc73-4eb08d8b6819",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File format: json\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import mimetypes\n",
    "\n",
    "# def get_file_format(file_path):\n",
    "#     # Get the file extension\n",
    "#     file_extension = file_path.split('.')[-1]\n",
    "#     return file_extension\n",
    "\n",
    "\n",
    "\n",
    "# # Example file path\n",
    "# file_path = \"dbfs:/FileStore/Nested_json_file__1_-1.json\"\n",
    "\n",
    "# # Get the file format\n",
    "# file_format = get_file_format(file_path)\n",
    "\n",
    "# # Show the file format\n",
    "# print(\"File format:\", file_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebabb0b-68db-4c02-916c-bd98bda43a99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.lang.RuntimeException: abort: DriverClient destroyed\n",
       "\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:618)\n",
       "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$2(NamedExecutor.scala:466)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:364)\n",
       "\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:457)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:75)\n",
       "\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:456)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.lang.RuntimeException: abort: DriverClient destroyed\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$3(DriverClient.scala:618)\n\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:54)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$2(NamedExecutor.scala:466)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:364)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:457)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:75)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:456)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, expr\n",
    "\n",
    "\n",
    "\n",
    "# # Sample data\n",
    "# data = [(\"John Doe\",), (\"Jane Smith\",), (\"Alice Johnson\",)]\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "# # Extract substring\n",
    "# df.withColumn(\"substring\", expr(\"substring(name, 1, 4)\")).show()\n",
    "\n",
    "# # Alternatively, you can use col function\n",
    "# df.withColumn(\"substring\", col(\"name\").substr(1, 4)).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3e394f-4078-4dfa-9417-3131b3443591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ProcessorTest",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
